# Spec Readiness Checklist

> 20-item assessment rubric used by the SDD Companion GPT (Mode 3: Spec Readiness Assessment). Each item is scored Red/Yellow/Green. The checklist determines whether an idea is ready for `/speckit.specify`.

---

## Scoring Guide

| Score | Meaning | Action |
|-------|---------|--------|
| ğŸŸ¢ **Green** | Fully addressed. Evidence or clarity is sufficient. | Ready for spec |
| ğŸŸ¡ **Yellow** | Partially addressed. Some gaps but not blocking. | Can proceed with `[NEEDS CLARIFICATION]` markers in spec |
| ğŸ”´ **Red** | Missing or critically insufficient. | Must be resolved before specifying |

**Readiness threshold**:
- **Ready**: 0 Reds, â‰¤3 Yellows â†’ Generate `/speckit.specify` prompt
- **Almost Ready**: 0 Reds, 4-6 Yellows â†’ Generate prompt with prominent `[NEEDS CLARIFICATION]` markers
- **Not Ready**: Any Reds â†’ Provide specific remediation steps, do not generate prompt

---

## The Checklist

### Problem Clarity (Items 1-4)

**1. Problem statement is specific and evidence-based**
- ğŸŸ¢ Clear "[User] experiences [problem] when [context], resulting in [impact]" format with evidence
- ğŸŸ¡ Problem described but vague on impact or evidence
- ğŸ”´ No clear problem statement, or pure solution description

**2. Target user segment is identified**
- ğŸŸ¢ Specific user role/segment named with context (e.g., "PMs managing FX taxonomy billing codes")
- ğŸŸ¡ User identified but too broad (e.g., "product managers")
- ğŸ”´ No user identified, or "everyone"

**3. Current workaround is documented**
- ğŸŸ¢ Specific current process described with pain points
- ğŸŸ¡ General description of current state
- ğŸ”´ No information about how users handle this today

**4. Impact is quantified**
- ğŸŸ¢ Measurable impact stated (time, errors, cost, frequency)
- ğŸŸ¡ Impact described qualitatively but not measured
- ğŸ”´ No impact assessment

### Scope Definition (Items 5-8)

**5. V1 scope is explicitly bounded**
- ğŸŸ¢ Clear list of what's included AND what's excluded
- ğŸŸ¡ Inclusions listed but exclusions not addressed
- ğŸ”´ Scope is open-ended or not defined

**6. API surface is sketched**
- ğŸŸ¢ Endpoints, methods, and high-level request/response described
- ğŸŸ¡ API interaction described conceptually but not as endpoints
- ğŸ”´ No API surface consideration

**7. System interactions are mapped**
- ğŸŸ¢ Upstream/downstream systems identified with impact
- ğŸŸ¡ Some systems mentioned but interaction unclear
- ğŸ”´ No system interaction analysis

**8. Backward compatibility impact assessed**
- ğŸŸ¢ Breaking vs. non-breaking changes identified, migration considered
- ğŸŸ¡ Acknowledged but not analyzed in detail
- ğŸ”´ Not considered, or feature has obvious compatibility implications that are unaddressed

### Assumptions & Risks (Items 9-12)

**9. Key assumptions are documented**
- ğŸŸ¢ Assumptions listed with confidence and impact ratings
- ğŸŸ¡ Some assumptions identified but not rated
- ğŸ”´ No assumptions documented (everything presented as fact)

**10. High-risk assumptions have validation plans**
- ğŸŸ¢ Each high-risk assumption has a specific validation approach
- ğŸŸ¡ Validation plans exist for some but not all high-risk assumptions
- ğŸ”´ No validation plans for high-risk assumptions

**11. Compliance risks are identified**
- ğŸŸ¢ SOX/audit/approval implications explicitly assessed
- ğŸŸ¡ Compliance mentioned but not analyzed
- ğŸ”´ Compliance not considered for a feature that touches financial data

**12. Rollback/failure scenarios considered**
- ğŸŸ¢ Failure modes identified with mitigation strategies
- ğŸŸ¡ Some failure scenarios mentioned
- ğŸ”´ No failure scenario analysis

### Success Criteria (Items 13-16)

**13. Success metrics are measurable**
- ğŸŸ¢ Specific, measurable outcomes defined (e.g., "reduce time from 4h to 15min")
- ğŸŸ¡ Outcomes described but not measurable
- ğŸ”´ No success metrics, or vanity metrics only

**14. Acceptance criteria are testable**
- ğŸŸ¢ Given/When/Then scenarios for happy path and key edge cases
- ğŸŸ¡ Acceptance criteria described but not in testable format
- ğŸ”´ No acceptance criteria

**15. Performance constraints are stated**
- ğŸŸ¢ Response time, throughput, and data volume limits specified
- ğŸŸ¡ Performance mentioned but not quantified
- ğŸ”´ No performance requirements for a feature with obvious performance implications

**16. Non-functional requirements addressed**
- ğŸŸ¢ Availability, scalability, security, auditability explicitly covered
- ğŸŸ¡ Some non-functional requirements mentioned
- ğŸ”´ Non-functional requirements not considered

### Technical & Stakeholder Readiness (Items 17-20)

**17. Tech lead has assessed feasibility**
- ğŸŸ¢ Tech lead confirmed feasibility and identified technical risks
- ğŸŸ¡ Informal tech discussion but no formal assessment
- ğŸ”´ No technical feasibility check

**18. Domain model impact understood**
- ğŸŸ¢ New/modified entities, fields, and relationships documented
- ğŸŸ¡ Domain impact described conceptually
- ğŸ”´ Domain model impact not considered

**19. DACI roles assigned**
- ğŸŸ¢ Driver, Approver, Contributors, Informed all named
- ğŸŸ¡ Some roles assigned
- ğŸ”´ No role assignment

**20. Stakeholder alignment confirmed**
- ğŸŸ¢ Key stakeholders briefed and aligned
- ğŸŸ¡ Some stakeholder discussions held
- ğŸ”´ No stakeholder engagement

---

## Output Format

The GPT produces a scored checklist followed by a summary and recommendation:

```
## Spec Readiness Assessment: [Feature Name]

### Scores

| # | Item | Score | Notes |
|---|------|-------|-------|
| 1 | Problem statement | ğŸŸ¢ | Clear problem with evidence from support tickets |
| 2 | Target user segment | ğŸŸ¢ | "PMs managing FX taxonomy billing codes" |
| 3 | Current workaround | ğŸŸ¢ | Manual one-by-one entry described |
| ... | ... | ... | ... |

### Summary
- **Green**: 14/20
- **Yellow**: 4/20
- **Red**: 2/20

### Recommendation
**Not Ready** â€” 2 items require resolution before specifying:
- Item 11 (Compliance risks): Feature creates billing codes â€” SOX implications must be assessed
- Item 17 (Tech feasibility): No tech lead input on CSV parsing at scale

### Remediation Steps
1. Schedule 30-min session with compliance lead to assess SOX implications
2. Get tech lead confirmation on CSV processing limits and error handling approach
3. Re-run assessment after resolving red items

### [If Ready] Draft /speckit.specify Prompt
[Only included if threshold is met â€” see spec-readiness-to-specify bridge below]
```

---

## Spec Readiness â†’ `/speckit.specify` Bridge

When the checklist passes (0 Reds, â‰¤3 Yellows), the GPT generates a draft `/speckit.specify` prompt that:

1. **Opens with the problem** (from Items 1-4)
2. **Defines requirements** (from Items 5-8, including API surface)
3. **Lists acceptance criteria** (from Items 13-16, in Given/When/Then format)
4. **States constraints** (from Items 9-12, including compliance requirements)
5. **References constitution articles** (based on feature type â€” see constitution-guide.md quick reference)
6. **Marks open items** with `[NEEDS CLARIFICATION]` for Yellow items
7. **Provides domain context** (relevant entities from the domain model)

The PM reviews and edits this prompt before running it through spec-kit. The GPT's draft is a starting point, not a finished product.
